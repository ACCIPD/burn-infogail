import numpy as np
import itertools
import datetime

from rllab.config import PROJECT_PATH
import argparse

parser = argparse.ArgumentParser()
parser.add_argument("--environment",type=str,default="DEBUG")
parser.add_argument("--repeats",type=int,default=10)
parser.add_argument("--reverse",type=int,default=0)
args = parser.parse_args()

now = datetime.datetime.now()

reward_epochs = [1, 50] # normal gan vs. wgan
base_name = "{}-{:02d}{:02d}{:02d}{:02d}".format(args.environment,now.month,now.day,now.hour,now.minute)

args_and_vars = \
    {
     #"save_policy":[1],
     "wgan":[0],
     "bc_init":[0],
     #"standard_prior":[0],
     #"reward_epoch":reward_epochs[],
     "debug":[0],
     "invdyn_start_epoch":[0],
     #"invdyn_temperature":[0.1,1.0,1.9],
     "exp_name":[base_name],
     #"environment":[args.environment]
     }

more_args_and_vars = {
#    "JNGSIM":{
#        #"info_recur_dim":[0, 64],
#        "policy_recur_dim":[0],
#        "environment":["JTZM"],
#        "mix_data_classes":[1],
#        #"use_reward_mul":[1,0],
#        "policy_merge":["concat"],
#        "z_policy_merge_idx":[2],
#        "policy_hidden_sizes":[[128, 64, 64, 32]],
#        "z_policy_hidden_sizes":[[]],
#        "reward_hidden_sizes":[[64]],
#        "info_hidden_sizes":[[64, 32]],
#        "info_reg":[0.1],
#        #"info_reg":[0.01],
#        "z_dim":[4],
#        #"learning_rate":[0.000001],
#        "trainer":["adam"],
#        "policy_nonlinearity":["tanh"],
#        "info_nonlinearity":["tanh"],
#        #"info_cell":["gru","lstm"],
#        ("info_recur_dim","info_cell"):zip([0, 32, 32],["none", "gru", "lstm"]),
#        "info_epoch":[15]
#        },
#    "JBOTH":{
#        #"info_recur_dim":[0, 64],
#        "policy_recur_dim":[0, 32],
#        "environment":["JNGSIM", "JTZM"],
#        "mix_data_classes":[1],
#        #"use_reward_mul":[1,0],
#        "policy_merge":["concat"],
#        "z_policy_merge_idx":[2],
#        "policy_hidden_sizes":[[128, 64, 64, 32]],
#        "z_policy_hidden_sizes":[[]],
#        "reward_hidden_sizes":[[32]],
#        "info_hidden_sizes":[[64, 32]],
#        "info_reg":[0.1],
#        #"info_reg":[0.01],
#        "z_dim":[4],
#        #"learning_rate":[0.000001],
#        "trainer":["adam"],
#        "policy_nonlinearity":["tanh"],
#        "info_nonlinearity":["tanh"],
#        #"info_cell":["gru","lstm"],
#        ("info_recur_dim", "info_cell"):zip([0,32,32],["none","gru"]),
#        "info_epoch":[15, 30, 45]
#        },
    "DEBUG":{
        "end_on_failure":[0,1],
        "rew_aug":[1.0],
        "n_itr":[150],
        "domain_indices":[[0]],
        "max_path_length":[50],
        'curr_add':[0],
        'curr_step':[1],
        ("model_all","curr_start"):zip([1,1,0],[2,1,1]),
        "environment":["JTZM"],
        "mix_data_classes":[1],
        #"policy_merge":["concat"],
        "policy_hidden_sizes":[[64, 64]],
        "reward_hidden_sizes":[[128, 64]],
        "z_dim":[0],
        "use_infogail":[0],
        "reward_learning_rate":[2e-5],
        "policy_nonlinearity":["tanh"],
        "policy_cell":["none"],
        "trpo_batch_size":[100 * 100],
        "reward_batch_size":[10],
        "wgan":[1],
        "reward_trainer":["adam"],
        "adaptive_std":[0],
        "index_features":[0,1]
        },
    "BUFFER":{
        "use_infogail":[1],
        "n_itr":[100],
        "max_path_length":[50],
        "wgan":[1],
        "domain_indices":[[0,1,2,3]],
        "environment":["JTZM"],
        "mix_data_classes":[1],
        "policy_merge":["concat"],
        "z_policy_merge_idx":[3],
        "policy_hidden_sizes":[[128, 128, 16, 16]],
        "z_policy_hidden_sizes":[[16]],
        "reward_hidden_sizes":[[64]],
        "info_hidden_sizes":[[64, 64, 64]],
        "info_reg":[0.4, 0.6],
        ("info_cnf","cnf_hidden_sizes"):zip([0.5, 0.2, 0.0],[[32],[32],[]]),
        "info_drop_prob":[0.1],
        "z_dim":[4],
        "reward_learning_rate":[5e-5],
        "info_learning_rate":[0.0001],
        "info_decay_rate":[0.97],
        "info_decay_step":[5],
        "policy_nonlinearity":["tanh"],
        "info_nonlinearity":["tanh"],
        ("info_recur_dim", "info_cell"):zip([0],["none"]),
        "policy_cell":["none"],
        #"trpo_batch_size":[120 * 100],
        "trpo_batch_size":[1 * 100],
        "reward_batch_size":[200],
        "info_batch_size":[100],
        "info_stochastic":[0],
        "info_epoch":[100],
        "reward_trainer":["rmsprop"],
        "info_trainer":["adam"],
        ("info_ent","use_info_prior"):zip([2e+3],[1]),
        "use_replay_buffer":[0,1],
        "include_cnf_in_reward":[0,1]
        },
    "EXPERIMENT1":{
        "use_infogail":[1],
        "n_itr":[100],
        "max_path_length":[50],
        #"info_recur_dim":[0, 64],
        "wgan":[1],
        "domain_indices":[[0,1,2,3]],
        #("environment","mix_data_classes"):zip(["JTZM","JTZS"],[1,0]),
        #"environment":["JTZS"],
        #"mix_data_classes":[0],
        #"use_reward_mul":[1,0],
        "environment":["JTZM"],
        "mix_data_classes":[1],
        "policy_merge":["concat"],
        "z_policy_merge_idx":[3],
        "policy_hidden_sizes":[[128, 128, 16, 16]],
        "z_policy_hidden_sizes":[[16]],
        "reward_hidden_sizes":[[64]],
        "info_hidden_sizes":[[64, 64, 64]],
        "info_reg":[0.4],
        ("info_cnf","cnf_hidden_sizes"):zip([0.5, 0.75, 0.0],[[32],[32],[]]),
        "info_drop_prob":[0.1],
        "z_dim":[4],
        "reward_learning_rate":[5e-5],
        "info_learning_rate":[0.0001],
        "info_decay_rate":[0.97],
        "info_decay_step":[5],
        "policy_nonlinearity":["tanh"],
        "info_nonlinearity":["tanh"],
        #"info_cell":["gru","lstm"],
        #("info_recur_dim", "info_cell"):zip([32,0],["lstm","none"]),
        ("info_recur_dim", "info_cell"):zip([0],["none"]),
        "policy_cell":["none"],
        "trpo_batch_size":[100 * 100],
        "reward_batch_size":[200],
        "info_batch_size":[100],
        "info_stochastic":[0],
        "info_epoch":[100],
        "reward_trainer":["rmsprop"],
        "info_trainer":["adam"],
        ("info_ent","use_info_prior"):zip([1e+3, 2e+3,0.,0.],[1,1,1,0])
        },
     "EXPERIMENT2":{
         "use_infogail":[1],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0, 0.25, 0.5],[[],[32],[32]]),
         "info_drop_prob":[0.1],
         "z_dim":[4],
         "reward_learning_rate":[5e-5],
         "info_learning_rate":[0.0001],
         "info_decay_rate":[0.97],
         "info_decay_step":[5],
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([2e+3, 0],[1, 1])
         },
     "EXPERIMENT3":{
         "use_infogail":[1],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         #("environment","mix_data_classes"):zip(["JTZM","JTZS"],[1,0]),
         #"environment":["JTZS"],
         #"mix_data_classes":[0],
         #"use_reward_mul":[1,0],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0, 0.75, 0.25, 0.5],[[],[32],[32],[32]]),
         "info_drop_prob":[0.1],
         "z_dim":[4],
         "reward_learning_rate":[5e-5],
         "info_learning_rate":[0.0001],
         "info_decay_rate":[0.97],
         "info_decay_step":[5],
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[64, 1024, 2048],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([2e+3],[1])
         },
     "EXPERIMENT4":{
         "use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         #("environment","mix_data_classes"):zip(["JTZM","JTZS"],[1,0]),
         #"environment":["JTZS"],
         #"mix_data_classes":[0],
         #"use_reward_mul":[1,0],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.9, 0.75, 0.5],[[32],[32],[32]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97,5],[0.95,10]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100, 150],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([2e+3],[1])
         },
     "EXPERIMENT5":{
         "use_infogail":[1],
         #"save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         #("environment","mix_data_classes"):zip(["JTZM","JTZS"],[1,0]),
         #"environment":["JTZS"],
         #"mix_data_classes":[0],
         #"use_reward_mul":[1,0],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.9, 0.75, 0.9,
             0.75],[[32],[32],[32,32],[32,32]]),
         "info_drop_prob":[0.1],
         "z_dim":[4],
         "reward_learning_rate":[5e-5],
         "info_learning_rate":[0.0001],
         "info_decay_rate":[0.97],
         "info_decay_step":[5],
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0, 16],["none", "lstm"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[1024],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([2e+3],[1])
         },
     "EXPERIMENT6":{
        "save_models":["policy","info"],
         "use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([0],[0])
         },
     "CORL1":{
        "save_models":["policy","info"],
         "use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[1]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[100 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([200, 0, 0],[1, 1, 0])
         },
     "CORL2":{
        "save_models":["policy","info"],
         "use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[1]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[100 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("info_ent","use_info_prior"):zip([2000, 1000, 500, 0, 0],[1, 1, 1, 1, 0])
         },
     "CORL3":{
         #"save_models":["policy","info"],
         #"use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[1]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0],["none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[100 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100],
         "info_stochastic":[0],
         "info_epoch":[100],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         ("use_infogail","info_ent","use_info_prior"):zip([0,1,1,1],[0,500, 0, 0],[0,1, 1, 0])
         },
     "CORL4":{
         #"save_models":["policy","info"],
         #"use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[1]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh", "relu"],
         ("info_recur_dim", "info_cell"):zip([64, 64, 0],["lstm","gru","none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[100 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[200],
         "info_stochastic":[0],
         "info_epoch":[200],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         "use_infogail":[1],
         "use_info_prior":[1],
         "info_ent":[500., 1000., 1.]
         #("use_infogail","info_ent","use_info_prior"):zip([1,1,1],[500, 0,
         #    0],[1, 1, 0])
         },
     "CORL5":{
         #"save_models":["policy","info"],
         #"use_infogail":[1],
         "save_models":[["policy","info"]],
         "n_itr":[500],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[1]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16]],
         "reward_hidden_sizes":[[64]],
         "info_hidden_sizes":[[64, 64]],
         "info_reg":[0.2,0.4],
         ("info_cnf","cnf_hidden_sizes"):zip([0.0],[[]]),
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":[0.0001],
         #"info_decay_rate":[0.97],
         #"info_decay_step":[5],
         ("info_decay_rate","info_decay_step"):zip([0.97],[5]),
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh", "relu"],
         ("info_recur_dim", "info_cell"):zip([64, 0],["lstm","none"]),
         "policy_cell":["none"],
         "trpo_batch_size":[100 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[200],
         "info_stochastic":[0],
         "info_epoch":[200],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam"],
         "include_cnf_in_reward":[0],
         "use_replay_buffer":[0],
         "use_infogail":[1],
         "use_info_prior":[1],
         "info_ent":[500., 1000., 1.]
         #("use_infogail","info_ent","use_info_prior"):zip([1,1,1],[500, 0,
         #    0],[1, 1, 0])
         },
     "SEARCH1":{
         "use_infogail":[1],
         #"save_models":[],
         "n_itr":[100],
         "max_path_length":[50],
         #"info_recur_dim":[0, 64],
         "wgan":[1],
         "domain_indices":[[0,1,2,3]],
         "environment":["JTZM"],
         "mix_data_classes":[1],
         "policy_merge":["concat"],
         "z_policy_merge_idx":[3, 1, 2, 0],
         "policy_hidden_sizes":[[128, 128, 16, 16]],
         #("z_policy_merge_idx","policy_hidden_sizes","z_policy_hidden_sizes"):zip([0,3],[[128,128],[128,128,16,16]],[[],[16]]),
         "z_policy_hidden_sizes":[[16], [32], [64], []],
         "reward_hidden_sizes":[[64], [64, 64]],
         "info_hidden_sizes":[[64, 64, 64], [128, 128], [64], [32,32,32,32]],
         "info_reg":[0.4],
         #("info_cnf","cnf_hidden_sizes"):zip([0.9],[[32]]),
         "info_cnf":[0.1,0.25,0.5,0.75,0.9],
         "info_drop_prob":[0.0],
         "z_dim":[4],
         "info_learning_rate":list(np.random.uniform(1e-5,1e-3,(30,))),
         "info_decay_rate":list(np.random.uniform(0.9,1.0,(6,))),
         "info_decay_step":[5, 10, 15],
         "policy_nonlinearity":["tanh"],
         "info_nonlinearity":["tanh"],
         #"info_cell":["gru","lstm"],
         ("info_recur_dim", "info_cell"):zip([0, 16, 32, 16, 32],["none","gru","gru","lstm","lstm"]),
         "policy_cell":["none"],
         "trpo_batch_size":[250 * 100, 100 * 100, 400 * 100],
         "reward_batch_size":[200],
         "info_batch_size":[100, 200, 300, 500],
         "info_stochastic":[0],
         "info_epoch":[50, 100, 150, 200, 250],
         "reward_trainer":["rmsprop"],
         "info_trainer":["adam", "rmsprop"],
         "include_cnf_in_reward":[0,1],
         "use_replay_buffer":[0],
         #("info_ent","use_info_prior"):zip([0],[0])
         "info_ent":list(np.random.uniform(1000,6000,(10,))),
         "use_info_prior":[1]
         },
    "PP":{
        "g":[25,30,35],
        "max_torque":[10]
    },
    "RS":{
        "wheel_moment_of_inertia":[4000],
        "invdyn_hidden_sizes":[[32,32,32]],
        "reward_hidden_sizes":[[32,32]],
        "policy_hidden_sizes":[[32,32,8]],
        "max_path_length":[200],
    },
    "LL":{
        "nonlinearity":["tanh"],
        "main_engine_power":[13.], # 13
        "side_engine_power":[0.6], # 0.6
        #("main_engine_power","side_engine_power"):zip(np.linspace(13.-6,13.+6,5),np.linspace(0.6-0.3,0.9-0.3,5)),
        #"invdyn_hidden_sizes":[[32,32,32]]
    }
}

args_and_vars.update(more_args_and_vars[args.environment])

for key, val in args_and_vars.items():
    if "_sizes" in key:
        assert type(val[0]) == list

arguments = args_and_vars.keys()
assignments = map(list,list(itertools.product(*args_and_vars.values())))

def unpack_nested_tuples(xx):
    istuple = lambda x : type(x) == tuple
    not_tuple = lambda x : not istuple(x)
    xx = filter(not_tuple,xx) + list(itertools.chain(*filter(istuple,xx)))
    return xx

arguments = unpack_nested_tuples(arguments)
assignments = [unpack_nested_tuples(assignment) for assignment in assignments]
#import pdb; pdb.set_trace()
if args.reverse:
    assignments = assignments[::-1]

print("PREPARING {} JOBS".format(len(assignments) * args.repeats))

s = ''
for _ in range(args.repeats):
    for assignment in assignments:
        d = dict(zip(arguments, assignment))
        s += 'python {}/myscripts/imitate_w_invdyn.py'.format(PROJECT_PATH)
        for ar, val in d.items():
            if val is None: continue
            if val == []: continue
            s += ' --{} {}'.format(ar,val)

        exp_name = "{}".format(d["environment"])
        #exp_name = "{}-BCINIT{}".format(d["environment"],d["bc_init"])
        if "exp_name" in d.keys():
            exp_name = "{}-".format(d["exp_name"]) + exp_name

	s += ' --{} {}'.format('reward_epoch',reward_epochs[d["wgan"]])
	s += ' --{} {}; '.format('exp_name',exp_name)

# remove bad chars
for char in ['[',']',',']:
    s = s.replace(char,'')
#print(s)

with open("{}/jobs/{}.sh".format(PROJECT_PATH, base_name),'a') as f:
    f.write(s)

